{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeehHLT3dMql"
      },
      "source": [
        "## Installing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXItbb1_neya",
        "outputId": "0b23c66e-fe5f-44d6-b9e2-61311e5e0d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Nt5ntCXEe8",
        "outputId": "df3b1a8d-751d-4ae6-cbd2-27c392c40938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting APScheduler\n",
            "  Downloading APScheduler-3.10.4-py3-none-any.whl (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler) (1.16.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from APScheduler) (2023.4)\n",
            "Requirement already satisfied: tzlocal!=3.*,>=2.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler) (5.2)\n",
            "Installing collected packages: APScheduler\n",
            "Successfully installed APScheduler-3.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install APScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY_ZF8g9-0Xz",
        "outputId": "122bc4e6-aa24-4a58-939a-d08aa511479b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC4ilnBZe7jp"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FVZe8zQgPnX6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Function to fetch news from Dawn website\n",
        "def fetch_news_dawn(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    articles = []\n",
        "    for item in soup.find_all('article'):\n",
        "        title = item.find('h2').text.strip() if item.find('h2') else 'No title'\n",
        "\n",
        "        # Extracting date string\n",
        "        date_tag = item.find('span', class_='timestamp--time')\n",
        "        date_str = date_tag['title'] if date_tag and 'title' in date_tag.attrs else None\n",
        "\n",
        "        # Convert date_str to datetime\n",
        "        if date_str:\n",
        "            date = pd.to_datetime(date_str)\n",
        "        else:\n",
        "            date = None\n",
        "\n",
        "        content_tag = item.find('div', class_='story__excerpt')\n",
        "        if content_tag:\n",
        "            content = content_tag.text.strip()\n",
        "        else:\n",
        "            content = 'No content'\n",
        "\n",
        "        # Only add articles with non-empty content\n",
        "        if content != 'No content':\n",
        "            articles.append({'title': title, 'content': content, 'date_str': date_str, 'date': date})\n",
        "\n",
        "    return articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA6wZUylfiz7"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TVnaHLelfSlo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "\n",
        "    text = text.lower()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWYVGF0Sggn1"
      },
      "source": [
        "## Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zsc4PWKkfpXw"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "\n",
        "# Initialize the classifier pipeline with a fine-tuned model\n",
        "classifier = pipeline('text-classification', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "# Function to classify news based on content\n",
        "def classify_news(text):\n",
        "    return classifier(text)[0]['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JBtGEAbgwLC"
      },
      "source": [
        "## Adding Subcategories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5WgSRZKTfpgo"
      },
      "outputs": [],
      "source": [
        "# Function to determine subcategory based on cleaned content\n",
        "def determine_subcategory(row):\n",
        "    text = row['cleaned_content']\n",
        "\n",
        "    # Sports subcategories\n",
        "    if 'cricket' in text:\n",
        "        return 'Cricket'\n",
        "    elif 'football' in text or 'soccer' in text:\n",
        "        return 'Football'\n",
        "    elif 'hockey' in text:\n",
        "        return 'Hockey'\n",
        "    elif 'tennis' in text:\n",
        "        return 'Tennis'\n",
        "    elif 'basketball' in text:\n",
        "        return 'Basketball'\n",
        "    elif 'golf' in text:\n",
        "        return 'Golf'\n",
        "    elif 'rugby' in text:\n",
        "        return 'Rugby'\n",
        "    elif 'athletics' in text:\n",
        "        return 'Athletics'\n",
        "\n",
        "    # Business subcategories\n",
        "    elif 'crypto' in text or 'cryptocurrency' in text or 'bitcoin' in text:\n",
        "        return 'Crypto'\n",
        "    elif 'stock' in text or 'market' in text or 'exchange' in text:\n",
        "        return 'Stock Exchanges'\n",
        "    elif 'finance' in text or 'economy' in text or 'bank' in text or 'investment' in text:\n",
        "        return \"Pakistan's Financial News\"\n",
        "    elif 'real estate' in text or 'property' in text:\n",
        "        return 'Real Estate'\n",
        "    elif 'trade' in text or 'commerce' in text:\n",
        "        return 'Trade'\n",
        "    elif 'oil' in text or 'gas' in text or 'energy' in text:\n",
        "        return 'Energy'\n",
        "    elif 'technology' in text or 'tech' in text or 'innovation' in text:\n",
        "        return 'Technology'\n",
        "    elif 'startup' in text or 'entrepreneur' in text:\n",
        "        return 'Startups'\n",
        "\n",
        "    # Culture subcategories\n",
        "    elif 'music' in text or 'showbiz' in text or 'celebrity' in text or 'film' in text or 'movie' in text or 'theater' in text:\n",
        "        return 'Music/Showbiz/Celebs'\n",
        "    elif 'art' in text or 'gallery' in text or 'painting' in text or 'sculpture' in text:\n",
        "        return 'Art'\n",
        "    elif 'literature' in text or 'book' in text or 'poetry' in text or 'novel' in text or 'author' in text:\n",
        "        return 'Literature'\n",
        "    elif 'fashion' in text or 'style' in text or 'design' in text:\n",
        "        return 'Fashion'\n",
        "    elif 'food' in text or 'cuisine' in text or 'restaurant' in text:\n",
        "        return 'Food'\n",
        "\n",
        "    # Politics subcategories\n",
        "    elif 'politic' in text or 'government' in text or 'election' in text or 'policy' in text:\n",
        "        return 'Politics'\n",
        "    elif 'corruption' in text or 'scandal' in text or 'bribery' in text:\n",
        "        return 'Corruption'\n",
        "    elif 'law' in text or 'court' in text or 'justice' in text:\n",
        "        return 'Law'\n",
        "    elif 'diplomacy' in text or 'foreign policy' in text:\n",
        "        return 'Diplomacy'\n",
        "\n",
        "    # International subcategories\n",
        "    elif 'international' in text or 'world' in text or 'foreign' in text:\n",
        "        return 'International'\n",
        "    elif 'asia' in text or 'china' in text or 'india' in text or 'japan' in text or 'korea' in text:\n",
        "        return 'Asia'\n",
        "    elif 'europe' in text or 'european' in text or 'germany' in text or 'france' in text or 'uk' in text or 'england' in text:\n",
        "        return 'Europe'\n",
        "    elif 'america' in text or 'us' in text or 'usa' in text or 'canada' in text:\n",
        "        return 'America'\n",
        "    elif 'africa' in text or 'african' in text:\n",
        "        return 'Africa'\n",
        "    elif 'middle east' in text or 'arab' in text or 'iran' in text or 'saudi' in text:\n",
        "        return 'Middle East'\n",
        "    elif 'oceania' in text or 'australia' in text or 'new zealand' in text:\n",
        "        return 'Oceania'\n",
        "\n",
        "    # National subcategories\n",
        "    elif 'pakistan' in text or 'pakistani' in text:\n",
        "        return 'National'\n",
        "\n",
        "    # Technology subcategories\n",
        "    elif 'tech' in text or 'technology' in text or 'innovation' in text:\n",
        "        return 'Technology'\n",
        "    elif 'ai' in text or 'artificial intelligence' in text or 'machine learning' in text or 'data science' in text:\n",
        "        return 'AI/ML/Data Science'\n",
        "    elif 'software' in text or 'programming' in text or 'coding' in text:\n",
        "        return 'Software Development'\n",
        "    elif 'hardware' in text or 'computer' in text or 'electronics' in text:\n",
        "        return 'Hardware'\n",
        "    elif 'internet' in text or 'web' in text or 'online' in text:\n",
        "        return 'Internet'\n",
        "    elif 'social media' in text or 'facebook' in text or 'twitter' in text or 'instagram' in text:\n",
        "        return 'Social Media'\n",
        "\n",
        "    # Health subcategories\n",
        "    elif 'health' in text or 'medicine' in text or 'hospital' in text or 'doctor' in text or 'nurse' in text:\n",
        "        return 'Health'\n",
        "    elif 'covid' in text or 'coronavirus' in text or 'pandemic' in text:\n",
        "        return 'COVID-19'\n",
        "    elif 'mental health' in text or 'psychology' in text or 'therapy' in text:\n",
        "        return 'Mental Health'\n",
        "    elif 'nutrition' in text or 'diet' in text or 'fitness' in text:\n",
        "        return 'Nutrition/Fitness'\n",
        "\n",
        "    # Education subcategories\n",
        "    elif 'education' in text or 'school' in text or 'college' in text or 'university' in text or 'student' in text:\n",
        "        return 'Education'\n",
        "    elif 'scholarship' in text or 'grant' in text or 'fellowship' in text:\n",
        "        return 'Scholarships/Grants'\n",
        "\n",
        "    # Environment subcategories\n",
        "    elif 'environment' in text or 'climate' in text or 'pollution' in text or 'conservation' in text:\n",
        "        return 'Environment'\n",
        "    elif 'wildlife' in text or 'animal' in text or 'biodiversity' in text:\n",
        "        return 'Wildlife'\n",
        "    elif 'sustainability' in text or 'renewable' in text or 'green' in text:\n",
        "        return 'Sustainability'\n",
        "\n",
        "    # Miscellaneous subcategories\n",
        "    elif 'travel' in text or 'tourism' in text or 'vacation' in text:\n",
        "        return 'Travel/Tourism'\n",
        "    elif 'automobile' in text or 'car' in text or 'bike' in text:\n",
        "        return 'Automobile'\n",
        "    elif 'space' in text or 'nasa' in text or 'astronomy' in text:\n",
        "        return 'Space/Astronomy'\n",
        "\n",
        "    # Default to 'Other' if no specific subcategory found\n",
        "    else:\n",
        "        return 'Other'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7oWupadhKHO"
      },
      "source": [
        "## Similarity Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8dSaVIhzKhZR"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "transformer_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "# Function to create and train a Doc2Vec model\n",
        "def train_doc2vec_model(news_df):\n",
        "    tagged_data = [TaggedDocument(words=row['cleaned_content'].split(), tags=[str(i)]) for i, row in news_df.iterrows()]\n",
        "    doc2vec_model = Doc2Vec(vector_size=50, alpha=0.025, min_alpha=0.025, min_count=1, dm=1)\n",
        "    doc2vec_model.build_vocab(tagged_data)\n",
        "    for epoch in range(100):\n",
        "        doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
        "        doc2vec_model.alpha -= 0.0002\n",
        "        doc2vec_model.min_alpha = doc2vec_model.alpha\n",
        "    return doc2vec_model\n",
        "\n",
        "# Function to find the most similar news using Doc2Vec\n",
        "def find_most_similar_doc2vec(news_df, model):\n",
        "    def get_most_similar(doc):\n",
        "        inferred_vector = model.infer_vector(doc.split())\n",
        "        sims = model.dv.most_similar([inferred_vector], topn=len(news_df))\n",
        "        most_similar_idx = int(sims[0][0])\n",
        "        return news_df.loc[most_similar_idx, 'content']\n",
        "\n",
        "    news_df['most_similar_doc2vec'] = news_df['cleaned_content'].apply(get_most_similar)\n",
        "    return news_df\n",
        "\n",
        "# Function to compute embeddings using transformers\n",
        "def compute_transformer_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = transformer_model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Function to find the most similar news using transformers\n",
        "def find_most_similar_transformers(news_df, embeddings):\n",
        "    def get_most_similar(idx):\n",
        "        cosine_similarities = np.dot(embeddings, embeddings[idx]) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embeddings[idx]))\n",
        "        most_similar_idx = np.argmax(cosine_similarities)\n",
        "        return news_df.loc[most_similar_idx, 'content']\n",
        "\n",
        "    news_df['most_similar_transformers'] = [get_most_similar(i) for i in range(len(news_df))]\n",
        "    return news_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nunTZWz2o3DG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO0ete97hzJV"
      },
      "source": [
        "## Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "t6FBZ1AbhdfG"
      },
      "outputs": [],
      "source": [
        "    # ranking function based on most recent date\n",
        "    def rank_news(group):\n",
        "        return group.sort_values(by='date', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQJCnNTfi2wU"
      },
      "source": [
        "## Scheduler and Load Balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaJxU5MTh1RA",
        "outputId": "8e6408a8-02b3-4fec-cdf3-808f0daa2222"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Every 10 seconds do scheduled_task() (last run: [never], next run: 2024-07-09 08:58:13)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import schedule\n",
        "import time\n",
        "# Function to perform the entire task\n",
        "def scheduled_task():\n",
        "    # Fetch news data from Dawn website\n",
        "    url = 'https://www.dawn.com'\n",
        "    news_data = fetch_news_dawn(url)\n",
        "\n",
        "    # Create DataFrame from news data\n",
        "    news_df = pd.DataFrame(news_data)\n",
        "\n",
        "    # Remove rows with 'No content' in content column\n",
        "    news_df = news_df[news_df['content'] != 'No content'].reset_index(drop=True)\n",
        "\n",
        "    # Preprocess content\n",
        "    news_df['cleaned_content'] = news_df['content'].apply(preprocess_text)\n",
        "\n",
        "    # Classify news into categories\n",
        "    news_df['category'] = news_df['cleaned_content'].apply(classify_news)\n",
        "\n",
        "    # Determine subcategory\n",
        "    news_df['subcategory'] = news_df.apply(determine_subcategory, axis=1)\n",
        "\n",
        "    # Train Doc2Vec model and find most similar news\n",
        "    doc2vec_model = train_doc2vec_model(news_df)\n",
        "    news_df = find_most_similar_doc2vec(news_df, doc2vec_model)\n",
        "\n",
        "    # Compute transformer embeddings and find most similar news\n",
        "    embeddings = compute_transformer_embeddings(news_df['cleaned_content'].tolist())\n",
        "    news_df = find_most_similar_transformers(news_df, embeddings)\n",
        "\n",
        "    # Group by category, apply ranking function\n",
        "    ranked_news = news_df.groupby(['category']).apply(rank_news).reset_index(drop=True)\n",
        "\n",
        "    # Save ranked news to CSV\n",
        "    ranked_news.to_csv('ranked_news_data.csv', index=False)\n",
        "\n",
        "    # Save analysis of categories\n",
        "    analysis = news_df['category'].value_counts().to_frame().reset_index()\n",
        "    analysis.columns = ['category', 'count']\n",
        "    analysis.to_csv('news_analysis.csv', index=False)\n",
        "\n",
        "    # Print execution count for debugging\n",
        "    print(f\"Task executed at {time.ctime()}\")\n",
        "\n",
        "# Schedule the task to run every 10 seconds (for testing purposes)\n",
        "schedule.every(10).seconds.do(scheduled_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YZ6oEVyjY8g"
      },
      "source": [
        "## Run The Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38ggxRyLiFGS",
        "outputId": "f470c13f-56d5-4c2c-f792-2d791ad60fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task executed at Tue Jul  9 08:58:20 2024\n",
            "Task executed at Tue Jul  9 08:58:33 2024\n",
            "Task executed at Tue Jul  9 08:58:37 2024\n",
            "Task executed at Tue Jul  9 08:58:44 2024\n",
            "Task executed at Tue Jul  9 08:58:47 2024\n",
            "Task executed at Tue Jul  9 08:58:50 2024\n",
            "Task executed at Tue Jul  9 08:59:00 2024\n",
            "Task executed at Tue Jul  9 08:59:11 2024\n",
            "Task executed at Tue Jul  9 08:59:20 2024\n",
            "Task executed at Tue Jul  9 08:59:31 2024\n",
            "Task executed at Tue Jul  9 08:59:41 2024\n"
          ]
        }
      ],
      "source": [
        "# Function to run the scheduler and stop it based on a condition\n",
        "def run_scheduler():\n",
        "    start_time = time.time()\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(1)\n",
        "        if time.time() > start_time + 30:  # Stop after 30 seconds for testing\n",
        "            break\n",
        "\n",
        "# Run the scheduler\n",
        "run_scheduler()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
